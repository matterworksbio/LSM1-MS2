{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import selfies as sf\n",
    "from train_selfies import MS2Gen, MSDataModule\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm as tqdm\n",
    "from pyteomics import mass\n",
    "from rdkit import Chem\n",
    "#import rdkit molwt\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from msdatasets import MSDataset\n",
    "\n",
    "config = {\n",
    "    \"dataset_path\": \"datasets/\",\n",
    "    \"batch_size\": 2,\n",
    "}\n",
    "\n",
    "path = \"\" #datasets path\n",
    "\n",
    "# set cuda visible devices\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "# Load dataset\n",
    "dataset = MSDataModule(\n",
    "    config[\"dataset_path\"],\n",
    "    config[\"batch_size\"],\n",
    ")\n",
    "\n",
    "# Set up trainer and fit\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    precision=\"16-mixed\",\n",
    "    sync_batchnorm=True,\n",
    "    use_distributed_sampler=True,\n",
    "    max_epochs=30,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "\n",
    "\n",
    "model = MS2Gen.load_from_checkpoint(f'./trained_models/ms2_paper_generative_best.ckpt')\n",
    "model.cuda()\n",
    "model.temperature = 1.0\n",
    "model.num_sequence = 100\n",
    "\n",
    "pl.seed_everything(42)\n",
    "dataset.setup(stage='test')\n",
    "test_loader = dataset.test_dataloader()\n",
    "val_loader = dataset.val_dataloader()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of common adducts and their corresponding mass adjustments\n",
    "adducts_mass_adjustments = {\n",
    "    \"[M]+\": 0,                                        # No mass adjustment\n",
    "    \"[M+H]+\": -mass.calculate_mass(formula='H'),       # Subtract the mass of a proton\n",
    "    \"[M+Na]+\": -mass.calculate_mass(formula='Na'),     # Subtract the mass of a sodium ion\n",
    "    \"[M+K]+\": -mass.calculate_mass(formula='K'),       # Subtract the mass of a potassium ion\n",
    "    \"[M+NH4]+\": -mass.calculate_mass(formula='NH4'),   # Subtract the mass of an ammonium ion\n",
    "    \"[M+2H]2+\": -mass.calculate_mass(formula='H'),     # Subtract the mass of a proton, note this applies to the singly charged m/z\n",
    "    \"[M+H+Na]2+\": -(mass.calculate_mass(formula='H') + mass.calculate_mass(formula='Na')) / 2, # Average mass shift for hybrid adduct\n",
    "    \"[M+2Na]2+\": -mass.calculate_mass(formula='Na'),   # Subtract the mass of a sodium ion, note for singly charged m/z\n",
    "    \"[M-H]-\": mass.calculate_mass(formula='H'),        # Add the mass of a proton (as it is removed)\n",
    "    \"[M+Cl]-\": -mass.calculate_mass(formula='Cl'),     # Subtract the mass of a chloride ion\n",
    "    \"[M+FA]-\": -mass.calculate_mass(formula='CHO2'),   # Subtract the mass of a formate ion\n",
    "    \"[M+Br]-\": -mass.calculate_mass(formula='Br')      # Subtract the mass of a bromide ion\n",
    "}\n",
    "\n",
    "def evaluate_data(loader, adducts_mass_adjustments, top_k=[1, 10, 100], do_reorder=False):\n",
    "    pl.seed_everything(42)\n",
    "    # for casmi dataset, get the predictions\n",
    "    preds = []\n",
    "    targets = []\n",
    "    precursor_list = []\n",
    "    i = 0\n",
    "    \n",
    "    #get the predictions\n",
    "    for batch in tqdm(loader):\n",
    "        mz, inty, precursormz, selfies = (\n",
    "            batch[\"mz\"],\n",
    "            batch[\"inty\"],\n",
    "            batch[\"precursormz\"],\n",
    "            batch[\"selfies\"],\n",
    "        )\n",
    "        mz, inty, precursormz = mz.cuda(), inty.cuda(), precursormz.cuda()\n",
    "        bert_inputs = model.collator(model.tokenizer(selfies))\n",
    "        bert_inputs = {\n",
    "            k: v.cuda() for k, v in bert_inputs.items()\n",
    "        }  # move to device since default dict is on cpu\n",
    "        logits, hidden_states, z, reconstructed = model(\n",
    "            mz, inty, precursormz, bert_inputs, selfies, mode=\"val\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # decode all of the selfies, predicted and targets\n",
    "        reconstructed = [sf.decoder(x) for x in reconstructed]\n",
    "        selfies = [sf.decoder(x) for x in selfies]\n",
    "        \n",
    "        # convert smiles to canonical\n",
    "        reconstructed_list = []\n",
    "        for x in reconstructed:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(x)\n",
    "                reconstructed_list.append(Chem.MolToSmiles(mol)) # convert to canonical smiles\n",
    "            except:\n",
    "                reconstructed_list.append(x)\n",
    "        selfies_list = []\n",
    "        for x in selfies:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(x)\n",
    "                selfies_list.append(Chem.MolToSmiles(mol)) # convert to canonical smiles\n",
    "            except:\n",
    "                selfies_list.append(x)\n",
    "        \n",
    "        preds.append(reconstructed_list)\n",
    "        targets.extend(selfies_list)\n",
    "        precursor_list.append(precursormz.detach().cpu().numpy())\n",
    "    \n",
    "    # fix to weird bug that happends with append if odd values\n",
    "    if len(precursor_list[0] > 1):\n",
    "        precursor_list_replacement = []\n",
    "        for i in range(len(precursor_list)):\n",
    "            for j in range(len(precursor_list[i])):\n",
    "                precursor_list_replacement.append(precursor_list[i][j])\n",
    "       \n",
    "       \n",
    "        precursor_list = precursor_list_replacement\n",
    "    \n",
    "    # move the predictions all to the same file \n",
    "    batch_preds = np.empty((len(precursor_list), 100), dtype=object)\n",
    "\n",
    "    total_chunks = sum((len(batch) + 99) // 100 for batch in preds)  # This calculates the ceiling of len(batch)/100 for each batch\n",
    "    batch_preds = np.empty((total_chunks, 100), dtype=object)\n",
    "    j = 0\n",
    "    for batch in tqdm(preds):\n",
    "        # break up batch into groups of 100 since 100 predictions were made at a time\n",
    "        batch = np.array(batch)\n",
    "        for i in range(0, len(batch), 100):\n",
    "            batch_preds[j] = batch[i:i+100]\n",
    "            j += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    precursor_list = np.asarray(precursor_list).flatten()\n",
    "    targets = np.array(targets).flatten()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    top_1_scores = []\n",
    "    top_10_scores = []\n",
    "    top_100_scores = []\n",
    "    exact_1 = []\n",
    "    exact_10 = []\n",
    "    exact_100 = []\n",
    "    \n",
    "    similarities_list = []\n",
    "    reorder_list = []\n",
    "    diffs_list = []\n",
    "    \n",
    "    #  get results one spectrum at a time\n",
    "    for i in tqdm(range(len(precursor_list))):\n",
    "        # create a list of possible precursors + mass adjustments\n",
    "        possible_precursors = np.array([precursor_list[i] + adducts_mass_adjustments[adduct] for adduct in adducts_mass_adjustments])\n",
    "\n",
    "        # get the list of masses for the predictions \n",
    "        pred_masses = np.empty(shape=(len(batch_preds[i])))\n",
    "        for j in range(len(batch_preds[i])):\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(batch_preds[i][j])\n",
    "                pred_masses[j] = (Descriptors.ExactMolWt(mol))\n",
    "            except:\n",
    "                pred_masses[j] = 0\n",
    "        \n",
    "        # for each predicted mass, calculate the smallest difference between the predicted mass and the possible precursors\n",
    "        diffs = np.empty(shape=(len(pred_masses)))\n",
    "        for z in range(len(pred_masses)):\n",
    "            diffs[z] = np.min(np.abs(possible_precursors - pred_masses[z]))\n",
    "        # # reorder the predictions based on the smallest difference\n",
    "        \n",
    "        if do_reorder:\n",
    "            reorder = np.argsort(diffs) \n",
    "            \n",
    "            reorder_list.append(reorder)\n",
    "            diffs = diffs[reorder]\n",
    "            diffs_list.append(diffs)\n",
    "            \n",
    "            batch_preds[i] = batch_preds[i][reorder]\n",
    "        else:\n",
    "            diffs_list.append(diffs)\n",
    "            reorder_list.append(np.arange(len(batch_preds[i])))\n",
    "        \n",
    "        for k in top_k:  \n",
    "            top_fingerprints = []\n",
    "            exact_matches = []\n",
    "            for x in batch_preds[i][:k]:\n",
    "                try:\n",
    "                    mol = Chem.MolFromSmiles(x)\n",
    "                    top_fingerprints.append(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024))\n",
    "                except:\n",
    "                    pass\n",
    "            target_fingerprint = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(targets[i]), 2, nBits=1024)\n",
    "            \n",
    "            for x in batch_preds[i][:k]:\n",
    "                if x == targets[i]:\n",
    "                    exact_matches.append(1)\n",
    "                else:\n",
    "                    exact_matches.append(0)\n",
    "            \n",
    "            # calculate the tanimoto similarity between the top 10 predictions and the target\n",
    "            similarities = np.array([Chem.DataStructs.TanimotoSimilarity(target_fingerprint, x) for x in top_fingerprints])\n",
    "            if similarities.size == 0:\n",
    "                similarities = np.array([0])          \n",
    "            similarities_list.append(similarities)\n",
    "                \n",
    "            if k == 1:\n",
    "                top_1_scores.append(np.max(similarities))\n",
    "                # if any exact matches == 1, append 1 to exact_1\n",
    "                if 1 in exact_matches:\n",
    "                    exact_1.append(1)\n",
    "                else:\n",
    "                    exact_1.append(0)\n",
    "            elif k == 10:  \n",
    "                top_10_scores.append(np.max(similarities))\n",
    "                if 1 in exact_matches:\n",
    "                    exact_10.append(1)\n",
    "                else:\n",
    "                    exact_10.append(0)\n",
    "            elif k == 100:\n",
    "                top_100_scores.append(np.max(similarities))\n",
    "                if 1 in exact_matches:\n",
    "                    exact_100.append(1)\n",
    "                else:\n",
    "                    exact_100.append(0)\n",
    "    \n",
    "    results['top_1_mean'] = np.mean(top_1_scores)\n",
    "    results['top_10_mean'] = np.mean(top_10_scores)\n",
    "    results['top_100_mean'] = np.mean(top_100_scores)\n",
    "    results['top_1_std'] = np.std(top_1_scores)\n",
    "    results['top_10_std'] = np.std(top_10_scores)\n",
    "    results['top_100_std'] = np.std(top_100_scores)\n",
    "    results['top_100_median'] = np.median(top_100_scores)\n",
    "    results['top_100_over40'] = len([x for x in top_100_scores if x > 0.4]) / len(top_100_scores)\n",
    "    results['top_100_over65'] = len([x for x in top_100_scores if x > 0.65]) / len(top_100_scores)\n",
    "    results['top_100_over95'] = len([x for x in top_100_scores if x > 0.95]) / len(top_100_scores)\n",
    "    results['top_100_100'] = len([x for x in top_100_scores if x >= 1.]) / len(top_100_scores)\n",
    "    results['top_10_median'] = np.median(top_10_scores)\n",
    "    results['top_10_over40'] = len([x for x in top_10_scores if x > 0.4]) / len(top_10_scores)\n",
    "    results['top_10_over65'] = len([x for x in top_10_scores if x > 0.65]) / len(top_10_scores)\n",
    "    results['top_10_over95'] = len([x for x in top_10_scores if x > 0.95]) / len(top_10_scores)\n",
    "    results['top_10_100'] = len([x for x in top_10_scores if x >= 1.]) / len(top_10_scores)\n",
    "    results['top_1_median'] = np.median(top_1_scores)\n",
    "    results['top_1_over40'] = len([x for x in top_1_scores if x > 0.4]) / len(top_1_scores)\n",
    "    results['top_1_over65'] = len([x for x in top_1_scores if x > 0.65]) / len(top_1_scores)\n",
    "    results['top_1_over95'] = len([x for x in top_1_scores if x > 0.95]) / len(top_1_scores)\n",
    "    results['top_1_100'] = len([x for x in top_1_scores if x >= 1.]) / len(top_1_scores)\n",
    "    results['exact_1'] = np.mean(exact_1)\n",
    "    results['exact_10'] = np.mean(exact_10)\n",
    "    results['exact_100'] = np.mean(exact_100)\n",
    "    \n",
    " \n",
    "    return results, batch_preds, targets, precursor_list, similarities_list, reorder_list, diffs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to save all of these files\n",
    "def save_gen_results(path, model, dataset, results, reorder = False):\n",
    "    results_dict, batch_preds, targets, precursors, similarities, reorder, diffs = results\n",
    "    results_df = pd.DataFrame(results_dict, index=[0])\n",
    "    batch_preds_df = pd.DataFrame(batch_preds)\n",
    "    targets_df = pd.DataFrame(targets)\n",
    "    precursors_df = pd.DataFrame(precursors)\n",
    "    similarities_df = pd.DataFrame(similarities)\n",
    "    reorder_df = pd.DataFrame(reorder)\n",
    "    diffs_df = pd.DataFrame(diffs)\n",
    "    \n",
    "    if reorder:\n",
    "        reorder_string = 'rerank'\n",
    "    else:\n",
    "        reorder_string = 'no_rerank'\n",
    "    \n",
    "    results_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_results.csv', index=False)\n",
    "    batch_preds_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_batch_preds.csv', index=False)\n",
    "    targets_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_targets.csv', index=False)\n",
    "    precursors_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_precursors.csv', index=False)\n",
    "    similarities_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_similarities.csv', index=False)\n",
    "    reorder_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}_reorder.csv', index=False)\n",
    "    diffs_df.to_csv(f'{path}/{model}/{dataset}_{reorder_string}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder_list = [True]\n",
    "model_name = ''\n",
    "\n",
    "for i in range(len(reorder_list)):\n",
    "    # for each loader in the test set, get the predictions\n",
    "    test = test_loader[0]\n",
    "    test = iter(test)\n",
    "    casmi = test_loader[1]\n",
    "    casmi = iter(casmi)\n",
    "    disjoint = test_loader[2]\n",
    "    disjoint = iter(disjoint)\n",
    "    casmi_2017 = test_loader[3]\n",
    "    casmi_2017 = iter(casmi_2017)\n",
    "\n",
    "    casmi2017_results = evaluate_data(casmi_2017, adducts_mass_adjustments, top_k=[1, 10, 100], do_reorder=True)\n",
    "    save_gen_results('../results/gen/', f'{model_name}', 'casmi2017', casmi2017_results, reorder=reorder_list[i])\n",
    "\n",
    "    casmi_results = evaluate_data(casmi, adducts_mass_adjustments, top_k=[1, 10, 100], do_reorder=True)\n",
    "    save_gen_results('../results/gen/', f'{model_name}', 'casmi', casmi_results, reorder=reorder_list[i])\n",
    "\n",
    "    test_results = evaluate_data(test, adducts_mass_adjustments, top_k=[1, 10, 100], do_reorder=True)\n",
    "    save_gen_results('../results/gen/', f'{model_name}', 'unknown', test_results, reorder=reorder_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find top temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.5, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0]\n",
    "coeff_keys = ['top_100_median', 'top_10_median', 'top_1_median']\n",
    "results = {}\n",
    "\n",
    "for t in temperatures:\n",
    "    model = MS2Gen.load_from_checkpoint(f'./trained_models/{model_name}.ckpt')\n",
    "    model.cuda()\n",
    "    model.temperature = t\n",
    "\n",
    "    # load the the unknown validation dataset\n",
    "    valid_set = MSDataset(\n",
    "                f\"{path}/val/no_casmi_val.zarr\",\n",
    "                mode=\"gen\",\n",
    "                smiles_path=f\"{path}/smiles/no_casmi_val_smiles.csv\",\n",
    "            )\n",
    "\n",
    "    # seed numpy\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # get a subset of 250 samples from valid set\n",
    "    valid_set = torch.utils.data.Subset(valid_set, np.random.choice(len(valid_set), 250))\n",
    "    \n",
    "    # create a dataloader for the subset\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=1, num_workers=4)\n",
    "\n",
    "    valid_results = evaluate_data(valid_loader, adducts_mass_adjustments, top_k=[1, 10, 100])\n",
    "\n",
    "    # add median results of top 100, 10, and 1\n",
    "    results_coeff = 0\n",
    "    for key in coeff_keys:\n",
    "        results_coeff += valid_results[0][key]\n",
    "    \n",
    "    results[t] = results_coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make results into df then save\n",
    "temperature_sweep_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "temperature_sweep_results.to_csv(f'../results/gen/{model_name}/temperature_sweep_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
