{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torchmetrics.regression import R2Score\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    ModelSummary,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from msdatasets import MSDataset\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# from train import SSModel\n",
    "from pretrain_MAE import SSModel\n",
    "from models.utils import jaccard_index\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "# from train import SSModel\n",
    "original_sys_path = sys.path.copy()\n",
    "sys.path.append(\"./lsm/hf_pretrain\")\n",
    "from models.conditional_gpt2_model import ConditionalGPT2LMHeadModel\n",
    "from pretrain_smiles_bert import MolBert\n",
    "from pretrain_smiles_decoder import SelfiesDecoder\n",
    "sys_path = sys.path\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForMaskedLM,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import os\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "\n",
    "import selfies as sf\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../results/gen/leafy-leaf-979-beam/'\n",
    "# read in csvs with all of the generated smiles\n",
    "casmi_smiles = pd.read_csv(f'{path}/casmi/casmi_rerank_batch_preds.csv')\n",
    "casmi2017_smiles = pd.read_csv(f'{path}/casmi2017/casmi2017_rerank_batch_preds.csv')\n",
    "unknown_smiles = pd.read_csv(f'{path}/unknown/unknown_rerank_batch_preds.csv')\n",
    "\n",
    "# read in corresponding smiles targets\n",
    "casmi_targets = pd.read_csv(f'{path}/casmi/casmi_rerank_targets.csv')\n",
    "casmi2017_targets = pd.read_csv(f'{path}/casmi2017/casmi2017_rerank_targets.csv')\n",
    "unknown_targets = pd.read_csv(f'{path}/unknown/unknown_rerank_targets.csv')\n",
    "#rename columns in ttargets to ground_truth\n",
    "casmi_targets.rename(columns={'0':'ground_truth'}, inplace=True)\n",
    "casmi2017_targets.rename(columns={'0':'ground_truth'}, inplace=True)\n",
    "unknown_targets.rename(columns={'0':'ground_truth'}, inplace=True)\n",
    "\n",
    "# collapse these into one df. First column is the targets, and the remainder are the generated smiles, with first column named target\n",
    "casmi = pd.concat([casmi_targets, casmi_smiles], axis=1)\n",
    "casmi2017 = pd.concat([casmi2017_targets, casmi2017_smiles], axis=1)\n",
    "unknown = pd.concat([unknown_targets, unknown_smiles], axis=1)\n",
    "\n",
    "# add a row to beginning specifying where each set comes from\n",
    "casmi.insert(0, 'set', 'casmi')\n",
    "casmi2017.insert(0, 'set', 'casmi2017')\n",
    "unknown.insert(0, 'set', 'unknown')\n",
    "\n",
    "# merge into one\n",
    "df = pd.concat([casmi, casmi2017, unknown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/miniconda3/envs/lsm/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inport BeRT style encoder\n",
    "encoder_model = MolBert.load_from_checkpoint(\n",
    "    \"/home/gabriel/lsm_ms2/MS2_LSM/lsm/hf_pretrain/trained_models/selfies_bert_best.ckpt\"\n",
    ")\n",
    "encoder_model.mask_pct = 0.0\n",
    "encoder_model.eval()\n",
    "encoder_model.cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"zjunlp/MolGen-large\", max_len=256\n",
    ")\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_store = torch.empty((len(df), 101, 1024))\n",
    "pl.seed_everything(42)\n",
    "# make a simple training loop\n",
    "for i in tqdm(range(len(df))):\n",
    "    embeddings = torch.empty((101, 1024))\n",
    "    j = 0\n",
    "    for _ in range(1):\n",
    "        # extract last 100 rows\n",
    "        smiles = df.iloc[i, 1+j:1+j+101].values\n",
    "        if len(smiles) == 0 :\n",
    "            break\n",
    "        # convert smiles to selfies\n",
    "        selfies = []\n",
    "        for smile in smiles:\n",
    "            try:\n",
    "                selfies.append(sf.encoder(smile))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # encode the selfies\n",
    "        tokenized_selfies = tokenizer(\n",
    "                selfies,\n",
    "                padding=True,\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        inputs = collator(tokenized_selfies)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder_model(\n",
    "                input_ids=inputs['input_ids'], \n",
    "                attention_mask=inputs['attention_mask'], \n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        full_embeddings = outputs[1][-1]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        mean_embeddings = (full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(\n",
    "            -1\n",
    "        ).unsqueeze(-1)\n",
    "        embeddings[j:j+len(selfies)] = mean_embeddings\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Free up memory\n",
    "        del inputs, outputs, full_embeddings, mean_embeddings, tokenized_selfies\n",
    "        torch.cuda.empty_cache()  # Clear cache to free unused memory\n",
    "\n",
    "\n",
    "    embedding_store[i] = embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ground_truth'].iloc[365]\n",
    "gens = df.iloc[365, 2:].values\n",
    "target_selfie = sf.encoder(target)\n",
    "gen_selfies = []\n",
    "for smile in range(len(gens)):\n",
    "    try:\n",
    "        gen_selfies.append(sf.encoder(gens[smile]))\n",
    "    except:\n",
    "        print(smile)\n",
    "        pass\n",
    "\n",
    "s = [target_selfie, gen_selfies]\n",
    "embs = []\n",
    "for a in s:\n",
    "    tokenized_selfies = tokenizer(\n",
    "            a,\n",
    "            padding=True,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    inputs = collator(tokenized_selfies)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder_model(\n",
    "            input_ids=inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'], \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    full_embeddings = outputs[1][-1]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    mean_embeddings = (full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(\n",
    "        -1\n",
    "    ).unsqueeze(-1)\n",
    "    embs.append(mean_embeddings)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12981, 1024]) torch.Size([12981, 100, 1024])\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f'{path}/embeddings', exist_ok=True)\n",
    "# first column is target embeddings, next 100 colums are generated embeddings\n",
    "target_embeds = embedding_store[:, 0]\n",
    "generated_embeds = embedding_store[:, 1:]\n",
    "\n",
    "print(target_embeds.shape, generated_embeds.shape)\n",
    "\n",
    "# save the embeddings as a numpy array along with the corresponding targets\n",
    "np.save(f'{path}/embeddings/target_embeddings.npy', target_embeds.numpy())\n",
    "np.save(f'{path}/embeddings/generated_embeddings.npy', generated_embeds.numpy())\n",
    "df.to_csv(f'{path}/embeddings/targets_and_generated_smiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 1M random smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc100m = pd.read_parquet('/mnt/data/gabriel_data/ai_ds_gabriel_ms2_workspace/datasets/hf_smiles/zinc100m.parquet')\n",
    "# filter out 1M smiles\n",
    "zinc1m = zinc100m.sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple dataset\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]['SELFIES']\n",
    "    \n",
    "selfies_data = EmbeddingDataset(zinc1m)\n",
    "\n",
    "loader = DataLoader(\n",
    "    selfies_data,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_embedding_store = torch.empty((len(zinc1m),  1024))\n",
    "pl.seed_everything(42)\n",
    "\n",
    "j= 0\n",
    "for batch in tqdm(loader):\n",
    "    tokenized_selfies = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = collator(tokenized_selfies)\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder_model(\n",
    "            input_ids=inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'], \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    full_embeddings = outputs[1][-1]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    mean_embeddings = (full_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(\n",
    "        -1\n",
    "    ).unsqueeze(-1)\n",
    "    zinc_embedding_store[j:j+len(batch)] = mean_embeddings.cpu()\n",
    "    j += len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings as a column to the zinc1m dataframe\n",
    "zinc1m['embeddings'] = zinc_embedding_store.numpy().tolist()\n",
    "zinc1m.to_parquet(f'{path}/embeddings/zinc1m_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>zinc_id</th>\n",
       "      <th>SELFIES</th>\n",
       "      <th>deepsmiles</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45300403</th>\n",
       "      <td>CC1CCC(N(C(=O)COn2nnc3ccc(S(=O)(=O)N(C)C)cc32)...</td>\n",
       "      <td>500410842</td>\n",
       "      <td>[C][C][C][C][C][Branch2][Ring2][O][N][Branch2]...</td>\n",
       "      <td>CCCCCNC=O)COnnnccccS=O)=O)NC)C)))cc69)))))))))...</td>\n",
       "      <td>[0.5256675481796265, 0.3143625557422638, -0.38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90756616</th>\n",
       "      <td>Cc1cccc(NNC(=O)NC[C@@H](NS(=O)(=O)N(C)C)C2CCCC...</td>\n",
       "      <td>1636569630</td>\n",
       "      <td>[C][C][=C][C][=C][C][Branch2][Ring1][S][N][N][...</td>\n",
       "      <td>CcccccNNC=O)NC[C@@H]NS=O)=O)NC)C))))CCCCC5))))...</td>\n",
       "      <td>[-0.13607320189476013, 0.4302113652229309, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85566921</th>\n",
       "      <td>CO[C@@H](CN1CC[C@H]1CNC(=O)C1(C)CC=CC1)C1CCCC1</td>\n",
       "      <td>1754493515</td>\n",
       "      <td>[C][O][C@@H1][Branch2][Ring1][Branch2][C][N][C...</td>\n",
       "      <td>CO[C@@H]CNCC[C@H]4CNC=O)CC)CC=CC5)))))))))))))...</td>\n",
       "      <td>[0.5098204612731934, -0.12734751403331757, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50096874</th>\n",
       "      <td>C[C@H](c1nccn1C)n1cncc1[C@@H]1CCNC1</td>\n",
       "      <td>2061159209</td>\n",
       "      <td>[C][C@H1][Branch1][=Branch2][C][=N][C][=C][N][...</td>\n",
       "      <td>C[C@H]cnccn5C))))))ncncc5[C@@H]CCNC5</td>\n",
       "      <td>[0.6597678065299988, -0.21915557980537415, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55130686</th>\n",
       "      <td>CC[C@H]1[C@@H](O)CCN1C(=O)C[C@@H]1CC[C@@H]2C[C...</td>\n",
       "      <td>2251000541</td>\n",
       "      <td>[C][C][C@H1][C@@H1][Branch1][C][O][C][C][N][Ri...</td>\n",
       "      <td>CC[C@H][C@@H]O)CCN5C=O)C[C@@H]CC[C@@H]C[C@H]63</td>\n",
       "      <td>[0.7782033681869507, -0.051961664110422134, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20810104</th>\n",
       "      <td>C=C[C@](C)(CC)C(=O)NCC[C@H](C)NC(=O)[C@H]1CCCN1C</td>\n",
       "      <td>1078221725</td>\n",
       "      <td>[C][=C][C@][Branch1][C][C][Branch1][Ring1][C][...</td>\n",
       "      <td>C=C[C@]C)CC))C=O)NCC[C@H]C)NC=O)[C@H]CCCN5C</td>\n",
       "      <td>[0.29643702507019043, 0.05870760977268219, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11442819</th>\n",
       "      <td>COc1cccc(/C=C/C(=O)NC[C@@H]2OC[C@@H]3CCN(C(=O)...</td>\n",
       "      <td>1790999571</td>\n",
       "      <td>[C][O][C][=C][C][=C][C][Branch2][Ring2][Ring2]...</td>\n",
       "      <td>COccccc/C=C/C=O)NC[C@@H]OC[C@@H]CCNC=O)CCSC=N)...</td>\n",
       "      <td>[0.47248637676239014, 0.4747932553291321, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31598141</th>\n",
       "      <td>COC(=O)COc1cccc(CNC(=O)C=Cc2ccccc2C)c1</td>\n",
       "      <td>2297580483</td>\n",
       "      <td>[C][O][C][=Branch1][C][=O][C][O][C][=C][C][=C]...</td>\n",
       "      <td>COC=O)COcccccCNC=O)C=Ccccccc6C))))))))))))c6</td>\n",
       "      <td>[0.4460662305355072, -0.16228066384792328, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64222092</th>\n",
       "      <td>Cc1cnc(CNCC[C@H](C)NC(=O)c2sc(C)cc2C)cn1</td>\n",
       "      <td>1497195651</td>\n",
       "      <td>[C][C][=C][N][=C][Branch2][Ring1][=Branch2][C]...</td>\n",
       "      <td>CccncCNCC[C@H]C)NC=O)cscC)cc5C)))))))))))))cn6</td>\n",
       "      <td>[0.5250552892684937, -0.23900504410266876, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77090782</th>\n",
       "      <td>CC(C)NCc1cn(Cc2cccnc2O)cn1</td>\n",
       "      <td>714976009</td>\n",
       "      <td>[C][C][Branch1][C][C][N][C][C][=C][N][Branch1]...</td>\n",
       "      <td>CCC)NCccnCccccnc6O))))))))cn5</td>\n",
       "      <td>[0.9619973301887512, -0.21936003863811493, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     smiles     zinc_id  \\\n",
       "45300403  CC1CCC(N(C(=O)COn2nnc3ccc(S(=O)(=O)N(C)C)cc32)...   500410842   \n",
       "90756616  Cc1cccc(NNC(=O)NC[C@@H](NS(=O)(=O)N(C)C)C2CCCC...  1636569630   \n",
       "85566921     CO[C@@H](CN1CC[C@H]1CNC(=O)C1(C)CC=CC1)C1CCCC1  1754493515   \n",
       "50096874                C[C@H](c1nccn1C)n1cncc1[C@@H]1CCNC1  2061159209   \n",
       "55130686  CC[C@H]1[C@@H](O)CCN1C(=O)C[C@@H]1CC[C@@H]2C[C...  2251000541   \n",
       "...                                                     ...         ...   \n",
       "20810104   C=C[C@](C)(CC)C(=O)NCC[C@H](C)NC(=O)[C@H]1CCCN1C  1078221725   \n",
       "11442819  COc1cccc(/C=C/C(=O)NC[C@@H]2OC[C@@H]3CCN(C(=O)...  1790999571   \n",
       "31598141             COC(=O)COc1cccc(CNC(=O)C=Cc2ccccc2C)c1  2297580483   \n",
       "64222092           Cc1cnc(CNCC[C@H](C)NC(=O)c2sc(C)cc2C)cn1  1497195651   \n",
       "77090782                         CC(C)NCc1cn(Cc2cccnc2O)cn1   714976009   \n",
       "\n",
       "                                                    SELFIES  \\\n",
       "45300403  [C][C][C][C][C][Branch2][Ring2][O][N][Branch2]...   \n",
       "90756616  [C][C][=C][C][=C][C][Branch2][Ring1][S][N][N][...   \n",
       "85566921  [C][O][C@@H1][Branch2][Ring1][Branch2][C][N][C...   \n",
       "50096874  [C][C@H1][Branch1][=Branch2][C][=N][C][=C][N][...   \n",
       "55130686  [C][C][C@H1][C@@H1][Branch1][C][O][C][C][N][Ri...   \n",
       "...                                                     ...   \n",
       "20810104  [C][=C][C@][Branch1][C][C][Branch1][Ring1][C][...   \n",
       "11442819  [C][O][C][=C][C][=C][C][Branch2][Ring2][Ring2]...   \n",
       "31598141  [C][O][C][=Branch1][C][=O][C][O][C][=C][C][=C]...   \n",
       "64222092  [C][C][=C][N][=C][Branch2][Ring1][=Branch2][C]...   \n",
       "77090782  [C][C][Branch1][C][C][N][C][C][=C][N][Branch1]...   \n",
       "\n",
       "                                                 deepsmiles  \\\n",
       "45300403  CCCCCNC=O)COnnnccccS=O)=O)NC)C)))cc69)))))))))...   \n",
       "90756616  CcccccNNC=O)NC[C@@H]NS=O)=O)NC)C))))CCCCC5))))...   \n",
       "85566921  CO[C@@H]CNCC[C@H]4CNC=O)CC)CC=CC5)))))))))))))...   \n",
       "50096874               C[C@H]cnccn5C))))))ncncc5[C@@H]CCNC5   \n",
       "55130686     CC[C@H][C@@H]O)CCN5C=O)C[C@@H]CC[C@@H]C[C@H]63   \n",
       "...                                                     ...   \n",
       "20810104        C=C[C@]C)CC))C=O)NCC[C@H]C)NC=O)[C@H]CCCN5C   \n",
       "11442819  COccccc/C=C/C=O)NC[C@@H]OC[C@@H]CCNC=O)CCSC=N)...   \n",
       "31598141       COC=O)COcccccCNC=O)C=Ccccccc6C))))))))))))c6   \n",
       "64222092     CccncCNCC[C@H]C)NC=O)cscC)cc5C)))))))))))))cn6   \n",
       "77090782                      CCC)NCccnCccccnc6O))))))))cn5   \n",
       "\n",
       "                                                 embeddings  \n",
       "45300403  [0.5256675481796265, 0.3143625557422638, -0.38...  \n",
       "90756616  [-0.13607320189476013, 0.4302113652229309, -0....  \n",
       "85566921  [0.5098204612731934, -0.12734751403331757, -0....  \n",
       "50096874  [0.6597678065299988, -0.21915557980537415, -0....  \n",
       "55130686  [0.7782033681869507, -0.051961664110422134, -0...  \n",
       "...                                                     ...  \n",
       "20810104  [0.29643702507019043, 0.05870760977268219, 0.0...  \n",
       "11442819  [0.47248637676239014, 0.4747932553291321, 0.06...  \n",
       "31598141  [0.4460662305355072, -0.16228066384792328, -0....  \n",
       "64222092  [0.5250552892684937, -0.23900504410266876, -0....  \n",
       "77090782  [0.9619973301887512, -0.21936003863811493, 0.2...  \n",
       "\n",
       "[1000000 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zinc1m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
