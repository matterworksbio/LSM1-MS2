{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "sys.path.append(\"../../lsm/\")\n",
    "\n",
    "from train_spectral import Finetune_SSSpectral\n",
    "from msdatasets import MSDataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = '' # path to the data\n",
    "mode = 'test' # name of the testing dataset\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "train_df = pd.read_pickle(f'{path}/processed_data/train_df.pkl')\n",
    "test_df = pd.read_pickle(f'{path}/processed_data/{mode}_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ground-truth maximum possible similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate tanimoto similarity with cosine lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_res = pd.read_pickle(f'{path}/processed_data/{mode}_modified_cosine_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_smiles_col = 'Train_SMILES'\n",
    "test_smiles_col = 'Test_SMILES'\n",
    "tanimoto_sim = []\n",
    "\n",
    "for i in tqdm(range(len(cos_res))):\n",
    "    train_smiles = cos_res[train_smiles_col].iloc[i]\n",
    "    test_smiles = cos_res[test_smiles_col].iloc[i]\n",
    "    train_mol = Chem.MolFromSmiles(train_smiles)\n",
    "    test_mol = Chem.MolFromSmiles(test_smiles)\n",
    "    train_fp = AllChem.GetMorganFingerprintAsBitVect(train_mol, 2, nBits=2048)\n",
    "    test_fp = AllChem.GetMorganFingerprintAsBitVect(test_mol, 2, nBits=2048)\n",
    "    \n",
    "    tanimoto_sim.append(DataStructs.TanimotoSimilarity(train_fp, test_fp))\n",
    "\n",
    "cos_res['Tanimoto_Similarity'] = tanimoto_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get percentage of rows where tanimoto similarity is > 0.95 and > 0.6 respectively\n",
    "print(cos_res[cos_res['Tanimoto_Similarity'] > 0.6].shape[0]/cos_res.shape[0])\n",
    "print(cos_res[cos_res['Tanimoto_Similarity'] > 0.95].shape[0]/cos_res.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate tanimoto similarity results with ms2lsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is (742049, 5) dimensions\n",
      "Data is (12274, 5) dimensions\n"
     ]
    }
   ],
   "source": [
    "model_name = 'spectral' # specify model name to load ckpt\n",
    "model = Finetune_SSSpectral.load_from_checkpoint(f'{path}/trained_models/{model_name}_best.ckpt')\n",
    "\n",
    "train_dataset = MSDataset(dataset_path=f'{path}/train/final_train.zarr/', mode='spectral', tanimoto_path = f'{path}/tanimoto/train_tanimoto.pkl')\n",
    "dataset = MSDataset(dataset_path=f'{path}/test/{mode}.zarr/', mode='spectral', tanimoto_path = f'{path}/tanimoto/{mode}_tanimoto.pkl')\n",
    "\n",
    "# # #create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size= 128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size= 256,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = torch.empty(size=(0, 512))\n",
    "precursor_list = []\n",
    "smiles_list = []\n",
    "for batch in tqdm(train_dataloader):\n",
    "    mz, inty, _, precursormz, smiles = batch['mz'], batch['inty'], batch['mode'], batch['precursormz'], batch['smiles1']\n",
    "    \n",
    "    mz, inty, precursormz = mz.cuda(), inty.cuda(), precursormz.cuda()\n",
    "    emb = model(precursormz, mz, inty).detach().cpu()\n",
    "    train_embeddings = torch.cat((train_embeddings, emb), dim=0)\n",
    "    smiles_list.append(smiles)\n",
    "    precursor_list.append(precursormz.detach().cpu())\n",
    "print(train_embeddings.shape)\n",
    "train_smiles = np.array(smiles_list).flatten()\n",
    "# flatten train_smiles on an individual spectrum level\n",
    "train_smiles = np.array([item for sublist in train_smiles for item in sublist])\n",
    "precursors_list = torch.stack(precursor_list[:-1])\n",
    "precursors_list = precursors_list.flatten() \n",
    "precursors_list = torch.concat([precursors_list, precursor_list[-1].flatten()])\n",
    "precursors_list = precursors_list.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy embeddings of training data if they have already been generated\n",
    "if os.path.exists(f'{path}/embeddings/{model_name}_train_embeddings.npy'):\n",
    "    train_embeddings = np.load(f'{path}/embeddings/{model_name}_train_embeddings.npy')\n",
    "    train_smiles = np.load(f'{path}/embeddings/{model_name}_train_smiles.npy')\n",
    "    train_precursors =  np.load(f'{path}/embeddings/{model_name}_train_precursors.npy')\n",
    " \n",
    "    \n",
    "    train_embeddings = torch.from_numpy(train_embeddings)\n",
    "else: # otherwise save the training embeddings generated from last block\n",
    "    #turn embeddings into numpy array\n",
    "    numpy_embeddings = train_embeddings.numpy()\n",
    "    os.makedirs(f'{path}/embeddings', exist_ok=True)\n",
    "    # save embeddings\n",
    "    np.save(f'{path}/embeddings/{model_name}_train_embeddings.npy', numpy_embeddings)\n",
    "    np.save(f'{path}/embeddings/{model_name}_train_smiles.npy', train_smiles)\n",
    "    np.save(f'{path}/embeddings/{model_name}_train_precursors.npy', precursors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:23<00:00,  4.13it/s]\n",
      "/tmp/ipykernel_63288/1372953522.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_smiles_list = np.array(test_smiles_list).flatten()\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = torch.empty(size=(0, 512))\n",
    "test_precursors = []\n",
    "test_smiles_list = []\n",
    "for batch in tqdm(dataloader):\n",
    "    mz, inty, _, precursormz, smiles = batch['mz'], batch['inty'], batch['mode'], batch['precursormz'], batch['smiles1']\n",
    "    \n",
    "    mz, inty, precursormz = mz.cuda(), inty.cuda(), precursormz.cuda()\n",
    "    emb = model(precursormz, mz, inty).detach().cpu()\n",
    "    test_embeddings = torch.cat((test_embeddings, emb), dim=0)\n",
    "    \n",
    "    test_smiles_list.append(smiles)\n",
    "    test_precursors.append(precursormz.detach().cpu())\n",
    "test_smiles_list = np.array(test_smiles_list).flatten()\n",
    "# flatten test_smiles on an individual spectrum level\n",
    "test_smiles_list = np.array([item for sublist in test_smiles_list for item in sublist])\n",
    "#clear cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "test_precursors_list = torch.stack(test_precursors[:-1])\n",
    "test_precursors_list = test_precursors_list.flatten() \n",
    "test_precursors_list = torch.concat([test_precursors_list, test_precursors[-1].flatten()])\n",
    "test_precursors_list = test_precursors_list.numpy()\n",
    "\n",
    "train_embeddings_norm = F.normalize(train_embeddings, p=2, dim=1)\n",
    "test_embeddings_norm = F.normalize(test_embeddings, p=2, dim=1)\n",
    "\n",
    "sim_matrix = torch.mm(test_embeddings_norm, train_embeddings_norm.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12274/12274 [00:08<00:00, 1443.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows where tanimoto similarity is > 0.95: 0.21704415838357505\n",
      "Percentage of rows where tanimoto similarity is > 0.6: 0.3658139155939384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in tqdm(range(len(sim_matrix))):\n",
    "    threshold = 0.005\n",
    "\n",
    "\n",
    "    while True:\n",
    "        upper_bound = test_precursors_list[i] + threshold/2\n",
    "        lower_bound = test_precursors_list[i] - threshold/2\n",
    "        \n",
    "        # Create a mask identifying values within the range\n",
    "        mask = (train_precursors >= lower_bound) & (train_precursors <= upper_bound)\n",
    "\n",
    "        # Create a new array containing only the values within this range\n",
    "        indices = np.where(mask)    \n",
    "        if len(indices[0]) > 0:\n",
    "            cosine_sim = sim_matrix[i]\n",
    "            filtered = cosine_sim[indices]\n",
    "            \n",
    "            argmax = indices[0][filtered.argmax()]\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            threshold = threshold * 2\n",
    "    \n",
    "    mol1 = Chem.MolFromSmiles(train_smiles[argmax])\n",
    "    mol2 = Chem.MolFromSmiles(test_smiles_list[i])\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)\n",
    "    fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)\n",
    "    scores.append(DataStructs.TanimotoSimilarity(fp1, fp2))\n",
    "scores = np.array(scores)\n",
    "print(f\"Percentage of rows where tanimoto similarity is > 0.95: {len(scores[scores > 0.95])/len(scores)}\")\n",
    "print(f\"Percentage of rows where tanimoto similarity is > 0.6: {len(scores[scores > 0.6])/len(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_scores = cos_res['Tanimoto_Similarity']\n",
    "# create dataframe where first row is method ('LSM1-MS2', 'Cosine Similarity') and second row is the cosine scores\n",
    "df = pd.DataFrame(data={'LSM1-MS2': scores, 'Cosine Similarity': cosine_scores})\n",
    "# make results section for this \n",
    "os.makedirs('../../results/database_retrieval', exist_ok=True)\n",
    "\n",
    "names = {'test':'unknown', 'disjoint_test':'known', 'casmi':'casmi'}\n",
    "\n",
    "df.to_csv(f'../../results/database_retrieval/{names[mode]}_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
